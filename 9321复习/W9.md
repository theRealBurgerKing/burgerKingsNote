# 第一节：Decision Trees
## 第一张：优点

决策树的主要优势：

### 1. 处理分类变量

- 可以直接处理类别型特征（如颜色、性别），无需转换为数值
- 不像线性回归那样要求所有特征必须是数值型

### 2. 处理缺失值和未知标签

- 数据中有缺失值时不需要删除整行数据
- 算法可以自动处理或绕过缺失的属性值

### 3. 检测非线性关系

- 能够捕捉特征之间复杂的非线性关系
- 不假设变量间存在线性关系，比线性模型更灵活

### 4. 决策树的可视化和解释

- 可以画成树状图，直观展示决策过程
- 业务人员和非技术人员也能轻松理解模型的决策逻辑

---

## 第二张：何时考虑使用决策树

### 适用场景

**实例可由属性-值对描述**

- 数据以特征-值的形式表示（如：年龄=30，收入=高）
- 适合结构化的表格数据

**目标函数是离散值**

- 预测的是类别标签，不是连续数值
- 例如：是/否、高/中/低等分类问题

**可能需要析取假设**

- 决策规则可能是"或"的关系（A或B或C）
- 可以表达复杂的逻辑组合条件

**可能存在噪声训练数据**

- 训练数据中有错误标记或异常值
- 决策树对噪声有一定的容忍度

**缺失属性值**

- 某些样本的某些特征值缺失
- 决策树可以在这些情况下继续工作

## 第三张：简介

### 核心思想

**使用决策树预测新事件的类别**

- 决策树像一个专家系统，通过提问逐步缩小范围
- 最终到达叶节点得到预测类别

**使用训练数据构建决策树**

- 从历史数据中学习如何提问和如何分类
- 自动找出最有区分度的特征和分割点

### 工作流程

```
训练阶段：
训练数据 → 构建决策树

预测阶段：
新事件 → 通过决策树 → 得到类别预测
```


## 决策树工作原理示例

### 简单例子：判断是否打网球

```
天气情况 → 决策树 → 是否打球

决策过程：
问题1：天气如何？
  ├─ 晴天 → 问题2：湿度如何？
  │         ├─ 高 → 不打 ✗
  │         └─ 低 → 打 ✓
  ├─ 阴天 → 打 ✓
  └─ 雨天 → 问题3：风大吗？
            ├─ 是 → 不打 ✗
            └─ 否 → 打 ✓
```

每个内部节点是一个问题，每个分支是一个答案，叶节点是最终决策。





# 决策树详解

## 第一张：决策树节点类型

### 决策树有两种节点

**1. 叶节点（Leaf Node）**

- 包含一个类别标签
- 由到达该叶节点的训练样本的多数投票决定
- 这是最终的决策结果

**2. 内部节点（Internal Node）**

- 是关于特征的一个问题
- 根据答案向不同方向分支
- 用于进一步划分数据


![](assets/W9/file-20251203202730977.png)




## 第一张：什么是好的决策树

### 核心问题

**决策树可能人类可读，但不使用人类逻辑！**

- 虽然看起来像人类的决策过程
- 但构建方法是算法驱动的，不是模仿人类思维

**如何构建能准确捕获数据的小型决策树？**

- 目标是树要小（简单）
- 同时要准确（高性能）

**学习最优决策树在计算上是难解的**

- 找到完美的决策树是NP完全问题
- 需要使用启发式方法近似求解

---

## 第二张：贪心算法

### 基本思想

==**我们可以通过简单的贪心算法获得好的决策树**==

- ==每次只考虑当前最好的选择==
- ==不回溯，不考虑全局最优==

**调整通常是为了修正贪心选择的问题**

- 贪心算法可能陷入局部最优
- 需要后续的剪枝等操作改进

---

### 递归构建过程

**步骤1：选择"最佳"变量**

- 选择最有区分度的特征
- 为每个可能的取值生成子节点

**步骤2：划分样本**

- 使用该特征的可能取值划分样本
- 将这些样本子集分配给对应的子节点

**步骤3：递归处理**

- 对每个子节点重复上述过程
- 直到某个节点的所有样本都是正类或都是负类



三种决策树：
合取
析取
异或

# ID3算法：决策树构建

## 第一张：ID3自顶向下归纳算法

### 算法步骤

**1. A ← 为下一个节点选择"最佳"决策属性**

- 评估所有候选属性
- 选择信息增益最大的

**2. 将A指定为该节点的决策属性**

- 该节点现在基于属性A进行分裂

**3. 对于A的每个值，创建新的后代节点**

- A有多少个可能值，就创建多少个子节点

**4. 根据分支的属性值将训练样本分配到叶节点**

- 每个样本根据其在A上的取值，被送到对应的子节点

**5. 检查是否完成**

- 如果所有训练样本都被完美分类（目标属性值相同）→ 停止
- 否则 → 在新的叶节点上迭代


==就是说每个节点都选择信息增益最大的==

---

## 第二张：变量选择

### 分割的最佳变量

**目标：最有信息量的变量**

- 选择对标签最有信息量的变量
- 信息量化方法由Claude Shannon提出

---

### 基本概念

![](assets/W9/file-20251203203633019.png)
---

#### 1. 熵（Entropy）

H(X)=−∑xP(X=x)log⁡P(X=x)H(X) = -\sum_x P(X=x) \log P(X=x)H(X)=−x∑​P(X=x)logP(X=x)

**含义**：

- ==衡量随机变量X的不确定性==
- ==数据越混乱，熵越高==
- ==数据越纯净，熵越低==

**范围**：

- 最小值：0（完全确定）
- 最大值：log₂(n)，n是类别数（二分类时最大为1）

---

#### 2. 条件熵（Conditional Entropy）

H(Y∣X)=∑xP(X=x)H(Y∣X=x)H(Y|X) = \sum_x P(X=x) H(Y|X=x)H(Y∣X)=x∑​P(X=x)H(Y∣X=x)

**含义**：

- ==给定特征X后，目标Y的剩余不确定性==
- ==知道X的信息后，关于Y还有多少不确定性==
- ==条件熵越小，说明X对预测Y越有帮助==

**计算过程**：

1. 对X的每个取值x，计算H(Y|X=x)
2. 按P(X=x)加权求和
3. 得到平均的条件熵

---

#### 3. 信息增益（Information Gain）

IG(Y;X)=H(Y)−H(Y∣X)IG(Y;X) = H(Y) - H(Y|X)IG(Y;X)=H(Y)−H(Y∣X)

**含义**：

- ==使用特征X后，关于Y的不确定性减少了多少==
- ==等于分割前的熵减去分割后的条件熵==
- ==信息增益越大，特征X越重要==

**决策准则**： **选择信息增益最高的变量作为分割特征**

---

## 第三张：熵的定义（重复）

### 符号说明

**S**：训练样本的集合 **p₊**：正样本的比例 **p₋**：负样本的比例

### 熵的作用

==**熵衡量S的不纯度（impurity）**==

- ==不纯度 = 混乱程度 = 不确定性==
- ==纯净的数据集熵值低==
- ==混乱的数据集熵值高==

### 公式

Entropy(S)=−p+log⁡2p+−p−log⁡2p−\text{Entropy}(S) = -p_+ \log_2 p_+ - p_- \log_2 p_-Entropy(S)=−p+​log2​p+​−p−​log2​p−​

### 熵曲线

**图形特征**：

- 横轴p从0到1（正样本比例）
- 纵轴entropy从0到1
- 曲线呈倒U型（抛物线状）

**关键点**：

- **p=0**：熵=0（全是负样本，完全确定）
- **p=0.5**：熵=1（正负各半，最不确定）
- **p=1**：熵=0（全是正样本，完全确定）
![](assets/W9/file-20251203204310894.png)





# 决策树的高级问题

## 第一张：连续值属性

### 问题

**如何处理连续值属性？**

**示例**：Temperature = 24.50C

- 温度是连续变量，不是离散类别
- 不能直接用于ID3算法

---

### 解决方案：创建离散属性

**方法：设置阈值**

**示例**：

```
(Temperature > 20.00C) = {true, false}
```


**问题**：**阈值设在哪里？**
### 详细算法

**步骤1：排序并标记变化点**

```
15(No), 18(No) | 19(Yes), 22(Yes), 24(Yes) | 27(No)
              ↑                            ↑
           阈值1候选                    阈值2候选
```

**步骤2：计算每个候选阈值的信息增益**

```
候选1：T > 18.5
- 左边：[15, 18] → 2个No
- 右边：[19, 22, 24, 27] → 3个Yes, 1个No

候选2：T > 25.5
- 左边：[15, 18, 19, 22, 24] → 3个Yes, 2个No
- 右边：[27] → 1个No
```

**步骤3：选择最佳阈值**

- 计算两个候选的信息增益
- 选择信息增益更大的




## 第二张：未知属性值

### 问题

**如果某些样本的某些属性值缺失怎么办？**

---

### 解决方案

#### 方案1：忽略缺失值

```
✓ 仍然使用该训练样本
✓ 让它通过决策树
✓ 如果节点n测试属性A，为A的其他值（在节点n排序的其他样本中）分配最常见的值
```

**示例**：

```
节点测试Humidity
该样本Humidity缺失
其他到达该节点的样本：7个High，3个Normal
→ 将该样本的Humidity设为High（最常见）
```

---

#### 方案2：最常见值（目标相同）

```
✓ 在具有相同目标值的其他样本中
✓ 分配属性A的最常见值
```

**示例**：

```
样本的PlayTennis=Yes，但Humidity缺失
其他PlayTennis=Yes的样本：4个High，5个Normal
→ 将Humidity设为Normal（在Yes类中最常见）
```

---

#### 方案3：概率分配

```
✓ 为属性A的每个可能值vi分配概率pi
✓ 将样本的pi比例分配给树的每个后代
✓ 以同样方式对新样本分类
```

**示例**：

```
Humidity缺失
到达该节点的样本：60%为High，40%为Normal

处理方式：
- 0.6的样本"权重"走High分支
- 0.4的样本"权重"走Normal分支

最终预测：结合两个分支的结果，按权重加权
```






决策树最大的问题之一是过拟合

# 避免过拟合 - 核心要点

## 第一张：两种策略

### 预剪枝

**当分割无统计显著性时停止生长**

### 后剪枝（推荐）

**先生长完整树，然后剪枝**

---

## 选择最佳树

**三种方法**：

1. ❌ 训练数据性能 - 会选择过拟合的树
2. ✅ **独立验证集性能** - 推荐
3. ✅ **min(|tree| + |错误数|)** - MDL原则

---

## 第二张：具体方法

### 思路1：预剪枝

**当误差改善 < 阈值时停止**

### 思路2：后剪枝

**最弱链剪枝（REP）**

- 从大树开始
- 从叶到根评估每个节点
- 删除对验证性能影响最小的子树






# 第一节第二部分：随机森林


# 集成学习

## 第一张：集成学习概述

### 决策树的问题

**决策树可能简单，但常常过拟合且方差高**

---

### ==集成学习的核心思想==

==**多个头脑一起工作通常能取得更好的结果**==

**统计直觉**： 平均多个测量值可以得到更稳定可靠的结果

---

### 关键概念

**模型组合：模型集成**

- 结合多个模型的预测
- 通过投票或平均得到最终结果

**特点**：

- 强大
- 但增加了算法和模型复杂度

---

## 第二张：模型平均方法

### 三种主要方法

#### 1. Bagging（装袋法）

**Breiman, 1996**

**方法**：

- 对训练数据进行自助重采样（bootstrap）
- 在每个重采样版本上训练大型树
- 通过多数投票分类

**特点**：

- 减少方差
- 并行训练
- 每棵树独立

---

#### 2. Boosting（提升法）

**Freund & Shapire, 1996**

**方法**：

- 训练多棵大树或小树
- 使用重新加权的训练数据版本
- 通过加权多数投票分类

**特点**：

- 顺序训练
- 每棵树关注前面树的错误
- 逐步提升性能

---

#### 3. Random Forests（随机森林）

**Breiman 1999**

**方法**： Bagging的更花哨版本

**关系**：

```
Boosting > Random Forests > Bagging > Single Tree
复杂度和效果递增
```

---

## 第三张：Bagging/Bootstrap聚合

### 可视化示例

**原始树（Original Tree）**：

- 在完整训练数据上训练的单棵树

**Bootstrap树1-5**：

- 每棵树在不同的bootstrap样本上训练
- 5棵树结构可能不同

---

### Bootstrap采样

**原理**：

```
原始数据：n个样本
Bootstrap样本：
- 有放回随机抽取n次
- 某些样本可能重复
- 某些样本可能不出现
```

---

### Bagging流程

```
1. 创建B个bootstrap样本
   ↓
2. 在每个样本上训练一棵树
   ↓
3. 得到B棵不同的树
   ↓
4. 新数据预测：
   - 让所有B棵树投票
   - 多数类获胜
```

---

### 为什么有效？

**减少方差**：

- 单棵树可能不稳定
- 数据微小变化 → 树结构大变
- 多棵树平均 → 稳定预测

**保持偏差**：

- 每棵树仍然是完整的树
- 拟合能力不降低

**数学原理**：

```
Var(平均) = Var(单个) / B

B棵树平均 → 方差减少B倍
```