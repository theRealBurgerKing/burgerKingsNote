# 第一节：Decision Trees
## 第一张：优点

决策树的主要优势：

### 1. 处理分类变量

- 可以直接处理类别型特征（如颜色、性别），无需转换为数值
- 不像线性回归那样要求所有特征必须是数值型

### 2. 处理缺失值和未知标签

- 数据中有缺失值时不需要删除整行数据
- 算法可以自动处理或绕过缺失的属性值

### 3. 检测非线性关系

- 能够捕捉特征之间复杂的非线性关系
- 不假设变量间存在线性关系，比线性模型更灵活

### 4. 决策树的可视化和解释

- 可以画成树状图，直观展示决策过程
- 业务人员和非技术人员也能轻松理解模型的决策逻辑

---

## 第二张：何时考虑使用决策树

### 适用场景

**实例可由属性-值对描述**

- 数据以特征-值的形式表示（如：年龄=30，收入=高）
- 适合结构化的表格数据

**目标函数是离散值**

- 预测的是类别标签，不是连续数值
- 例如：是/否、高/中/低等分类问题

**可能需要析取假设**

- 决策规则可能是"或"的关系（A或B或C）
- 可以表达复杂的逻辑组合条件

**可能存在噪声训练数据**

- 训练数据中有错误标记或异常值
- 决策树对噪声有一定的容忍度

**缺失属性值**

- 某些样本的某些特征值缺失
- 决策树可以在这些情况下继续工作

## 第三张：简介

### 核心思想

**使用决策树预测新事件的类别**

- 决策树像一个专家系统，通过提问逐步缩小范围
- 最终到达叶节点得到预测类别

**使用训练数据构建决策树**

- 从历史数据中学习如何提问和如何分类
- 自动找出最有区分度的特征和分割点

### 工作流程

```
训练阶段：
训练数据 → 构建决策树

预测阶段：
新事件 → 通过决策树 → 得到类别预测
```


## 决策树工作原理示例

### 简单例子：判断是否打网球

```
天气情况 → 决策树 → 是否打球

决策过程：
问题1：天气如何？
  ├─ 晴天 → 问题2：湿度如何？
  │         ├─ 高 → 不打 ✗
  │         └─ 低 → 打 ✓
  ├─ 阴天 → 打 ✓
  └─ 雨天 → 问题3：风大吗？
            ├─ 是 → 不打 ✗
            └─ 否 → 打 ✓
```

每个内部节点是一个问题，每个分支是一个答案，叶节点是最终决策。





# 决策树详解

## 第一张：决策树节点类型

### 决策树有两种节点

**1. 叶节点（Leaf Node）**

- 包含一个类别标签
- 由到达该叶节点的训练样本的多数投票决定
- 这是最终的决策结果

**2. 内部节点（Internal Node）**

- 是关于特征的一个问题
- 根据答案向不同方向分支
- 用于进一步划分数据


![](assets/W9/file-20251203202730977.png)




## 第一张：什么是好的决策树

### 核心问题

**决策树可能人类可读，但不使用人类逻辑！**

- 虽然看起来像人类的决策过程
- 但构建方法是算法驱动的，不是模仿人类思维

**如何构建能准确捕获数据的小型决策树？**

- 目标是树要小（简单）
- 同时要准确（高性能）

**学习最优决策树在计算上是难解的**

- 找到完美的决策树是NP完全问题
- 需要使用启发式方法近似求解

---

## 第二张：贪心算法

### 基本思想

==**我们可以通过简单的贪心算法获得好的决策树**==

- ==每次只考虑当前最好的选择==
- ==不回溯，不考虑全局最优==

**调整通常是为了修正贪心选择的问题**

- 贪心算法可能陷入局部最优
- 需要后续的剪枝等操作改进

---

### 递归构建过程

**步骤1：选择"最佳"变量**

- 选择最有区分度的特征
- 为每个可能的取值生成子节点

**步骤2：划分样本**

- 使用该特征的可能取值划分样本
- 将这些样本子集分配给对应的子节点

**步骤3：递归处理**

- 对每个子节点重复上述过程
- 直到某个节点的所有样本都是正类或都是负类



三种决策树：
合取
析取
异或

# ID3算法：决策树构建

## 第一张：ID3自顶向下归纳算法

### 算法步骤

**1. A ← 为下一个节点选择"最佳"决策属性**

- 评估所有候选属性
- 选择信息增益最大的

**2. 将A指定为该节点的决策属性**

- 该节点现在基于属性A进行分裂

**3. 对于A的每个值，创建新的后代节点**

- A有多少个可能值，就创建多少个子节点

**4. 根据分支的属性值将训练样本分配到叶节点**

- 每个样本根据其在A上的取值，被送到对应的子节点

**5. 检查是否完成**

- 如果所有训练样本都被完美分类（目标属性值相同）→ 停止
- 否则 → 在新的叶节点上迭代


==就是说每个节点都选择信息增益最大的==

---

## 第二张：变量选择

### 分割的最佳变量

**目标：最有信息量的变量**

- 选择对标签最有信息量的变量
- 信息量化方法由Claude Shannon提出

---

### 基本概念

![](assets/W9/file-20251203203633019.png)
---

#### 1. 熵（Entropy）

H(X)=−∑xP(X=x)log⁡P(X=x)H(X) = -\sum_x P(X=x) \log P(X=x)H(X)=−x∑​P(X=x)logP(X=x)

**含义**：

- ==衡量随机变量X的不确定性==
- ==数据越混乱，熵越高==
- ==数据越纯净，熵越低==

**范围**：

- 最小值：0（完全确定）
- 最大值：log₂(n)，n是类别数（二分类时最大为1）

---

#### 2. 条件熵（Conditional Entropy）

H(Y∣X)=∑xP(X=x)H(Y∣X=x)H(Y|X) = \sum_x P(X=x) H(Y|X=x)H(Y∣X)=x∑​P(X=x)H(Y∣X=x)

**含义**：

- ==给定特征X后，目标Y的剩余不确定性==
- ==知道X的信息后，关于Y还有多少不确定性==
- ==条件熵越小，说明X对预测Y越有帮助==

**计算过程**：

1. 对X的每个取值x，计算H(Y|X=x)
2. 按P(X=x)加权求和
3. 得到平均的条件熵

---

#### 3. 信息增益（Information Gain）

IG(Y;X)=H(Y)−H(Y∣X)IG(Y;X) = H(Y) - H(Y|X)IG(Y;X)=H(Y)−H(Y∣X)

**含义**：

- ==使用特征X后，关于Y的不确定性减少了多少==
- ==等于分割前的熵减去分割后的条件熵==
- ==信息增益越大，特征X越重要==

**决策准则**： **选择信息增益最高的变量作为分割特征**

---

## 第三张：熵的定义（重复）

### 符号说明

**S**：训练样本的集合 **p₊**：正样本的比例 **p₋**：负样本的比例

### 熵的作用

==**熵衡量S的不纯度（impurity）**==

- ==不纯度 = 混乱程度 = 不确定性==
- ==纯净的数据集熵值低==
- ==混乱的数据集熵值高==

### 公式

Entropy(S)=−p+log⁡2p+−p−log⁡2p−\text{Entropy}(S) = -p_+ \log_2 p_+ - p_- \log_2 p_-Entropy(S)=−p+​log2​p+​−p−​log2​p−​

### 熵曲线

**图形特征**：

- 横轴p从0到1（正样本比例）
- 纵轴entropy从0到1
- 曲线呈倒U型（抛物线状）

**关键点**：

- **p=0**：熵=0（全是负样本，完全确定）
- **p=0.5**：熵=1（正负各半，最不确定）
- **p=1**：熵=0（全是正样本，完全确定）
![](assets/W9/file-20251203204310894.png)





# 决策树的高级问题

## 第一张：连续值属性

### 问题

**如何处理连续值属性？**

**示例**：Temperature = 24.50C

- 温度是连续变量，不是离散类别
- 不能直接用于ID3算法

---

### 解决方案：创建离散属性

**方法：设置阈值**

**示例**：

```
(Temperature > 20.00C) = {true, false}
```


**问题**：**阈值设在哪里？**
### 详细算法

**步骤1：排序并标记变化点**

```
15(No), 18(No) | 19(Yes), 22(Yes), 24(Yes) | 27(No)
              ↑                            ↑
           阈值1候选                    阈值2候选
```

**步骤2：计算每个候选阈值的信息增益**

```
候选1：T > 18.5
- 左边：[15, 18] → 2个No
- 右边：[19, 22, 24, 27] → 3个Yes, 1个No

候选2：T > 25.5
- 左边：[15, 18, 19, 22, 24] → 3个Yes, 2个No
- 右边：[27] → 1个No
```

**步骤3：选择最佳阈值**

- 计算两个候选的信息增益
- 选择信息增益更大的




## 第二张：未知属性值

### 问题

**如果某些样本的某些属性值缺失怎么办？**

---

### 解决方案

#### 方案1：忽略缺失值

```
✓ 仍然使用该训练样本
✓ 让它通过决策树
✓ 如果节点n测试属性A，为A的其他值（在节点n排序的其他样本中）分配最常见的值
```

**示例**：

```
节点测试Humidity
该样本Humidity缺失
其他到达该节点的样本：7个High，3个Normal
→ 将该样本的Humidity设为High（最常见）
```

---

#### 方案2：最常见值（目标相同）

```
✓ 在具有相同目标值的其他样本中
✓ 分配属性A的最常见值
```

**示例**：

```
样本的PlayTennis=Yes，但Humidity缺失
其他PlayTennis=Yes的样本：4个High，5个Normal
→ 将Humidity设为Normal（在Yes类中最常见）
```

---

#### 方案3：概率分配

```
✓ 为属性A的每个可能值vi分配概率pi
✓ 将样本的pi比例分配给树的每个后代
✓ 以同样方式对新样本分类
```

**示例**：

```
Humidity缺失
到达该节点的样本：60%为High，40%为Normal

处理方式：
- 0.6的样本"权重"走High分支
- 0.4的样本"权重"走Normal分支

最终预测：结合两个分支的结果，按权重加权
```






决策树最大的问题之一是过拟合

# 避免过拟合 - 核心要点

## 第一张：两种策略

### 预剪枝

**当分割无统计显著性时停止生长**

### 后剪枝（推荐）

**先生长完整树，然后剪枝**

---

## 选择最佳树

**三种方法**：

1. ❌ 训练数据性能 - 会选择过拟合的树
2. ✅ **独立验证集性能** - 推荐
3. ✅ **min(|tree| + |错误数|)** - MDL原则

---

## 第二张：具体方法

### 思路1：预剪枝

**当误差改善 < 阈值时停止**

### 思路2：后剪枝

**最弱链剪枝（REP）**

- 从大树开始
- 从叶到根评估每个节点
- 删除对验证性能影响最小的子树






# 第一节第二部分：随机森林


# 集成学习

## 第一张：集成学习概述

### 决策树的问题

**决策树可能简单，但常常过拟合且方差高**

---

### ==集成学习的核心思想==

==**多个头脑一起工作通常能取得更好的结果**==

**统计直觉**： 平均多个测量值可以得到更稳定可靠的结果

---

### 关键概念

**模型组合：模型集成**

- 结合多个模型的预测
- 通过投票或平均得到最终结果

**特点**：

- 强大
- 但增加了算法和模型复杂度

---

## 第二张：模型平均方法

### 三种主要方法

#### 1. Bagging（装袋法）

**Breiman, 1996**

**方法**：

- 对训练数据进行自助重采样（bootstrap）
- 在每个重采样版本上训练大型树
- 通过多数投票分类

**特点**：

- 减少方差
- 并行训练
- 每棵树独立

==每个袋子有放回抽取固定数量样本，在每个bootstrap样本上独立训练一棵树，多数投票==

---

#### 2. Boosting（提升法）

**Freund & Shapire, 1996**

**方法**：

- 训练多棵大树或小树
- 使用重新加权的训练数据版本
- 通过加权多数投票分类

**特点**：

- 顺序训练
- 每棵树关注前面树的错误
- 逐步提升性能
==顺序训练多个弱学习器，每个学习器关注前面学习器的错误==
最终模型： - 所有树的加权组合 - 早期树权重大，后期树权重可能小



---

#### 3. Random Forests（随机森林）

**Breiman 1999**

**方法**： Bagging的更花哨版本

**关系**：

```
Boosting > Random Forests > Bagging > Single Tree
复杂度和效果递增
```

---













# 随机森林（Random Forest）

## 第一张：为什么需要随机森林

### Bagging的问题

**决策树的贪心特性**：

- 使用贪心算法选择分割变量
- 最小化误差
- ==即使使用Bagging，决策树仍有很多结构相似性==
- 导致预测之间高度相关

---

### ==集成学习的关键==

==**组合多个模型的预测在子模型预测不相关或弱相关时效果最好**==

==---==

### ==随机森林的改进==

==**改变子树的学习算法，使所有子树的预测相关性更低**==

---

## 第二张：随机森林的方法

### 同时对特征和样本进行Bagging

**三个关键机制**：

---

#### 1. 特征随机采样

==**在每次树分裂时，随机抽取m个特征**==

- 只考虑这m个特征用于分裂
- 通常 m=dm = \sqrt{d} m=d​ 或 log⁡2d\log_2 d log2​d（d是总特征数）

**示例**：

```
总特征：100个
每次分裂只看：√100 = 10个特征
```

---

#### 2. 袋外错误率（OOB）

==**对于每棵树在bootstrap样本上训练后**：==

- ==监控未被抽到的样本（袋外样本）的错误率==
- ==称为"袋外"错误率==

**作用**：

- 免费的验证集
- 无需单独划分数据

---

#### 3. 去相关策略

**随机森林通过"去相关"树来改进Bagging**

- 每棵树有相同的期望性能
- 但树之间更加独立
- 降低预测相关性








## 总结

### 随机森林 = Bagging + 特征随机化

==**核心改进**：==

```
Bagging：样本随机化
随机森林：样本随机化 + 特征随机化
```

**效果**：

```
更低的树间相关性
→ 更好的方差减少
→ 更高的准确率
```



# 随机森林的特点与优势

## 第一组优势

### 1. 高准确率

**最准确的学习算法之一**

- 在许多数据集上产生高度准确的分类器

### 2. 高效运行

**在大数据库上运行高效**

### 3. 处理高维数据

**可以处理数千个输入变量，无需变量删除**

---

## 第二组优势

### 4. 特征重要性

**给出变量在分类中的重要性估计**

- 自动排序特征重要性

### 5. 泛化误差估计

**在构建森林时生成泛化误差的内部无偏估计**

- 通过袋外（OOB）错误率实现

### 6. 处理缺失值

**有效估计缺失数据的方法**

- 即使大部分数据缺失也能保持准确性

---

## 第三组优势

### 7. 处理不平衡数据

**有方法平衡类别不平衡数据集中的错误**

### 8. 模型可重用

**生成的森林可以保存用于未来其他数据**

### 9. 可解释性

**计算的原型给出变量与分类之间关系的信息**



==**准确性**：最高准确率之一 **效率**：快速训练和预测 **鲁棒性**：处理缺失值、异常值、高维数据 **可解释**：特征重要性、原型分析 **实用性**：易用、可保存、适应性强==








# 第二节：Clustering

# 无监督学习（Unsupervised Learning）

## 第一张：定义

### 无监督学习的定义

**学习有用的结构，_没有_ 标记的类别、优化标准、反馈信号或原始数据之外的任何其他信息**

**关键词**：

- ❌ 无标签
- ❌ 无目标值
- ❌ 无反馈
- ✅ 只有原始数据

---

## 第二张：无监督学习详解

### 核心特点

**在没有标记响应或目标值的数据集上操作**

---

### 目标

**捕获感兴趣的结构或有用的信息**

- 例如：关系、模式、分组

---

### 应用场景

**1. 可视化复杂数据集的结构**

- 将高维数据降到2D/3D展示

**2. 压缩和总结数据**

- 例如：图像压缩

**3. 为监督学习提取特征**

- 特征工程的预处理步骤

**4. 发现分组或异常值**

- 聚类分析
- 异常检测

---

## 第三张：聚类示例

### 聚类（Clustering）

**无监督学习的典型应用**

**图示说明**：

- 散点图中的数据点被分成5个簇（cluster）
- 每个簇用不同颜色和形状表示
- ⭐ 星星标记表示各簇的中心点（centroids）