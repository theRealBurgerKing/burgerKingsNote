# 第一节：Decision Trees
## 第一张：优点

决策树的主要优势：

### 1. 处理分类变量

- 可以直接处理类别型特征（如颜色、性别），无需转换为数值
- 不像线性回归那样要求所有特征必须是数值型

### 2. 处理缺失值和未知标签

- 数据中有缺失值时不需要删除整行数据
- 算法可以自动处理或绕过缺失的属性值

### 3. 检测非线性关系

- 能够捕捉特征之间复杂的非线性关系
- 不假设变量间存在线性关系，比线性模型更灵活

### 4. 决策树的可视化和解释

- 可以画成树状图，直观展示决策过程
- 业务人员和非技术人员也能轻松理解模型的决策逻辑

---

## 第二张：何时考虑使用决策树

### 适用场景

**实例可由属性-值对描述**

- 数据以特征-值的形式表示（如：年龄=30，收入=高）
- 适合结构化的表格数据

**目标函数是离散值**

- 预测的是类别标签，不是连续数值
- 例如：是/否、高/中/低等分类问题

**可能需要析取假设**

- 决策规则可能是"或"的关系（A或B或C）
- 可以表达复杂的逻辑组合条件

**可能存在噪声训练数据**

- 训练数据中有错误标记或异常值
- 决策树对噪声有一定的容忍度

**缺失属性值**

- 某些样本的某些特征值缺失
- 决策树可以在这些情况下继续工作

## 第三张：简介

### 核心思想

**使用决策树预测新事件的类别**

- 决策树像一个专家系统，通过提问逐步缩小范围
- 最终到达叶节点得到预测类别

**使用训练数据构建决策树**

- 从历史数据中学习如何提问和如何分类
- 自动找出最有区分度的特征和分割点

### 工作流程

```
训练阶段：
训练数据 → 构建决策树

预测阶段：
新事件 → 通过决策树 → 得到类别预测
```


## 决策树工作原理示例

### 简单例子：判断是否打网球

```
天气情况 → 决策树 → 是否打球

决策过程：
问题1：天气如何？
  ├─ 晴天 → 问题2：湿度如何？
  │         ├─ 高 → 不打 ✗
  │         └─ 低 → 打 ✓
  ├─ 阴天 → 打 ✓
  └─ 雨天 → 问题3：风大吗？
            ├─ 是 → 不打 ✗
            └─ 否 → 打 ✓
```

每个内部节点是一个问题，每个分支是一个答案，叶节点是最终决策。





# 决策树详解

## 第一张：决策树节点类型

### 决策树有两种节点

**1. 叶节点（Leaf Node）**

- 包含一个类别标签
- 由到达该叶节点的训练样本的多数投票决定
- 这是最终的决策结果

**2. 内部节点（Internal Node）**

- 是关于特征的一个问题
- 根据答案向不同方向分支
- 用于进一步划分数据


![](assets/W9/file-20251203202730977.png)




## 第一张：什么是好的决策树

### 核心问题

**决策树可能人类可读，但不使用人类逻辑！**

- 虽然看起来像人类的决策过程
- 但构建方法是算法驱动的，不是模仿人类思维

**如何构建能准确捕获数据的小型决策树？**

- 目标是树要小（简单）
- 同时要准确（高性能）

**学习最优决策树在计算上是难解的**

- 找到完美的决策树是NP完全问题
- 需要使用启发式方法近似求解

---

## 第二张：贪心算法

### 基本思想

==**我们可以通过简单的贪心算法获得好的决策树**==

- ==每次只考虑当前最好的选择==
- ==不回溯，不考虑全局最优==

**调整通常是为了修正贪心选择的问题**

- 贪心算法可能陷入局部最优
- 需要后续的剪枝等操作改进

---

### 递归构建过程

**步骤1：选择"最佳"变量**

- 选择最有区分度的特征
- 为每个可能的取值生成子节点

**步骤2：划分样本**

- 使用该特征的可能取值划分样本
- 将这些样本子集分配给对应的子节点

**步骤3：递归处理**

- 对每个子节点重复上述过程
- 直到某个节点的所有样本都是正类或都是负类



三种决策树：
合取
析取
异或

# ID3算法：决策树构建

## 第一张：ID3自顶向下归纳算法

### 算法步骤

**1. A ← 为下一个节点选择"最佳"决策属性**

- 评估所有候选属性
- 选择信息增益最大的

**2. 将A指定为该节点的决策属性**

- 该节点现在基于属性A进行分裂

**3. 对于A的每个值，创建新的后代节点**

- A有多少个可能值，就创建多少个子节点

**4. 根据分支的属性值将训练样本分配到叶节点**

- 每个样本根据其在A上的取值，被送到对应的子节点

**5. 检查是否完成**

- 如果所有训练样本都被完美分类（目标属性值相同）→ 停止
- 否则 → 在新的叶节点上迭代


==就是说每个节点都选择信息增益最大的==

---

## 第二张：变量选择

### 分割的最佳变量

**目标：最有信息量的变量**

- 选择对标签最有信息量的变量
- 信息量化方法由Claude Shannon提出

---

### 基本概念

![](assets/W9/file-20251203203633019.png)
---

#### 1. 熵（Entropy）

H(X)=−∑xP(X=x)log⁡P(X=x)H(X) = -\sum_x P(X=x) \log P(X=x)H(X)=−x∑​P(X=x)logP(X=x)

**含义**：

- ==衡量随机变量X的不确定性==
- ==数据越混乱，熵越高==
- ==数据越纯净，熵越低==

**范围**：

- 最小值：0（完全确定）
- 最大值：log₂(n)，n是类别数（二分类时最大为1）

---

#### 2. 条件熵（Conditional Entropy）

H(Y∣X)=∑xP(X=x)H(Y∣X=x)H(Y|X) = \sum_x P(X=x) H(Y|X=x)H(Y∣X)=x∑​P(X=x)H(Y∣X=x)

**含义**：

- ==给定特征X后，目标Y的剩余不确定性==
- ==知道X的信息后，关于Y还有多少不确定性==
- ==条件熵越小，说明X对预测Y越有帮助==

**计算过程**：

1. 对X的每个取值x，计算H(Y|X=x)
2. 按P(X=x)加权求和
3. 得到平均的条件熵

---

#### 3. 信息增益（Information Gain）

IG(Y;X)=H(Y)−H(Y∣X)IG(Y;X) = H(Y) - H(Y|X)IG(Y;X)=H(Y)−H(Y∣X)

**含义**：

- ==使用特征X后，关于Y的不确定性减少了多少==
- ==等于分割前的熵减去分割后的条件熵==
- ==信息增益越大，特征X越重要==

**决策准则**： **选择信息增益最高的变量作为分割特征**

---

## 第三张：熵的定义（重复）

### 符号说明

**S**：训练样本的集合 **p₊**：正样本的比例 **p₋**：负样本的比例

### 熵的作用

==**熵衡量S的不纯度（impurity）**==

- ==不纯度 = 混乱程度 = 不确定性==
- ==纯净的数据集熵值低==
- ==混乱的数据集熵值高==

### 公式

Entropy(S)=−p+log⁡2p+−p−log⁡2p−\text{Entropy}(S) = -p_+ \log_2 p_+ - p_- \log_2 p_-Entropy(S)=−p+​log2​p+​−p−​log2​p−​

### 熵曲线

**图形特征**：

- 横轴p从0到1（正样本比例）
- 纵轴entropy从0到1
- 曲线呈倒U型（抛物线状）

**关键点**：

- **p=0**：熵=0（全是负样本，完全确定）
- **p=0.5**：熵=1（正负各半，最不确定）
- **p=1**：熵=0（全是正样本，完全确定）
![](assets/W9/file-20251203204310894.png)





